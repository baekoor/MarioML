üçÑ Enhanced Super Mario RL Training Framework
This enhanced framework builds upon the original Super Mario RL project to provide a more robust training pipeline with better tracking and visualization. The system allows for progressive training iterations, properly tracks performance metrics, and ensures proper GPU memory allocation.
Features

Progressive Training: Continue training from previous best models
VRAM Management: Properly allocates GPU memory for training
Performance Tracking: Comprehensive tracking of scores, loss, and stage progression
Visualization Tools: Plot training progress and compare different runs
Checkpointing: Regular saving of models to resume from crashes
Evaluation: Evaluate models with optional video recording

Getting Started
Prerequisites
Install the required dependencies:
bashCopypip install -r requirements.txt
Directory Structure
The framework will create several directories to organize outputs:

models/: Stores the best models from each run
checkpoints/: Stores intermediate checkpoints for resuming training
stats/: Stores training statistics and plots
videos/: Stores recorded gameplay videos during evaluation

Training Mario
Basic Training
To start a new training run:
bashCopypython progressive_mario_trainer.py --run-name my_first_run
Training Parameters
The training script supports many customization options:
bashCopypython progressive_mario_trainer.py --help
Key parameters include:

--num-episodes: Number of episodes to train (default: 10000)
--batch-size: Batch size for training (default: 256)
--learning-rate: Learning rate (default: 0.0001)
--eps-start, --eps-end, --eps-decay: Control exploration vs. exploitation
--gpu-memory-fraction: Control how much GPU memory to allocate (default: 0.8)

Continuing Training
To find the best model and set up the next training iteration:
bashCopypython continue_training.py --iteration 2
This will:

Find the best model from previous runs
Create a new checkpoint for the next iteration
Provide a command to run the next training iteration

For more control:
bashCopypython continue_training.py --model-path models/specific_model.pth --reset-optimizer --run-name mario_advanced
Visualizing Progress
View Training Progress
To visualize a training run:
bashCopypython visualize_training.py --run-name my_first_run
This generates plots showing:

Score progression
Loss values
Exploration rate decay
Stage progression

Compare Multiple Runs
To compare the performance of different training runs:
bashCopypython compare_runs.py --run-names run1 run2 run3 --save
Evaluating Models
To watch your trained agent play:
bashCopypython eval_mario.py --model-path models/my_best_model.pth --render
To record a video of your agent:
bashCopypython eval_mario.py --model-path models/my_best_model.pth --record
Additional evaluation options:

--plot: Generate a plot showing Mario's position over time
--max-steps: Limit the number of evaluation steps
--delay: Control playback speed for visualization

Architecture Overview
The framework uses a Dueling DQN architecture which separates state value and action advantage estimation for better performance. The neural network consists of:

Convolutional layers to process the game state
Separate value and advantage streams
Combined Q-value estimation

Training uses experience replay with a memory buffer to learn from past experiences and prioritize important transitions.
Tips for Successful Training

GPU Memory: Use --gpu-memory-fraction to control VRAM usage. For large memory GPUs, 0.8 is a good setting.
Training Iterations: Each iteration should typically be at least 1000 episodes to see meaningful progress.
Progressive Difficulty: As Mario learns basic navigation, consider reducing exploration rate for fine-tuning.
Checkpoint Frequency: For long training runs, increase --checkpoint-interval to avoid excessive disk I/O.
Hardware Requirements: Training works best with at least 8GB of RAM and a GPU with 4GB+ VRAM.

Acknowledgments
This project builds upon:

Original Super Mario RL implementation
OpenAI Gym
gym-super-mario-bros environment
PyTorch